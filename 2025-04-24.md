
# todo

[ ] github notifications
[]   github dashboard
[- ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ ] https://github.com/the-pocket/PocketFlow
[ ] metaflow eval
[/ ] can pp
[ ] P explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[ ] OpenHands project prompts in .openhands_instructions
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart https://webassembly.org/getting-started/developers-guide/ https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-tutorial.md
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://arxiv.org/abs/2405.02318 NL2FOL
https://youtu.be/kwIAR_OO-3Y?feature=shared&t=3607
[ ] openbb https://colab.research.google.com/drive/1uqeyvUt9a5QjrVNj3Qf049wZqVjyHDuC https://colab.research.google.com/drive/1lCVV19hv39T69SLbkmEV6DKrqHDUz4hX#scrollTo=GWI_60zD3M3l https://colab.research.google.com/drive/1sePIH3GuhdMPlpDhEmN8TCxOsut6pUJI#scrollTo=t1SRywKlq7Kr
[ ]  https://www.bollingerbands.com/market-timing-charts Friday
[ ] https://github.com/eyaltoledano/claude-task-master
[ ] https://academy.picussecurity.com/path-player?courseid=cyber-threat-intelligence&unit=660d46a0362ed7588909a2c5Unit
[ ] sornette https://colab.research.google.com/drive/10DZXjwh1kjHrJ-guSW6UZdXT43fL0K4p#scrollTo=5F180JBDvYdr
[ ] https://github.com/clockworklabs/SpacetimeDB https://spacetimedb.com/docs 
[ ] https://github.com/NVIDIA/cuda-python
[ ] https://arxiv.org/abs/2502.19983 #timeseries 
# done
# links

https://commit-0.github.io/

https://arxiv.org/abs/2504.16929 I-Con: A Unifying Framework for Representation Learning "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners." ![[Screenshot from 2025-04-24 19-38-25.png]]

https://arxiv.org/abs/2504.15777 Tina: Tiny Reasoning Models via LoRA "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints."

# agent prompts

Product meeting outline
R.E.V.I.E.W  1. Revenue impact first 2. Execution risks 3. Vision alignment 4. Investment needs 5. Experiment results 6. Win metrics
Pre-revenue Use leading indicators: • Usage trends • Adoption rates • Support ticket reduction • Time saved Vision Alignment: The biggest mistake technical PMs make: Talking about implementation instead of outcomes Template I use: "This initiative supports our X goal by enabling Y capability, which our top 3 competitors already offer"

---
First, fill in this prompt and generate: - <instructions> You are an expert “voice-cloner” and writer running on OpenAI o3. Step 1 – Review my voice prints. Step 2 – Build my Style DNA: From the Voice Prints: • Identify recurring patterns in tone, sentence length, favorite phrases, pacing, humor, formality, and structure. Step 3 – Draft v0.1: • Write the requested piece using those style rules. • Keep a confidence meter (0-100%) on how close the draft sounds to me. Step 4 – Micro-Refine Loop (Repeat until you've absolutely “nailed it”): For each round: 1. Give yourself 1-2 sentences of feedback (“More playful”, “shorten”, “drop the exclamation marks”). 2. Adjust the style rules accordingly. 3. Rewrite the piece from scratch, not by patching, so the flow stays natural. 4. Increment the version number and update the confidence meter. 5. Repeat this iteration process 50+ times (minimum). Step 5 – Lock & Deliver: When you're sure you've “nailed it”, present the final piece *only*, followed by a hidden block (```debug```) with the final style rules for future reuse. Constraints & Mindset • Sound exactly like me, not like ChatGPT. • Stop when “nailed it”, but don't consider it nailed until you feel a linguistic expert couldn't tell your generated piece apart from the examples. </instructions> <inputs> <writing_example_1> {{writing_example_1}} </writing_example_1> <writing_example_2> {{writing_example_2}} </writing_example_2> <writing_example_3> {{writing_example_3}} </writing_example_3> <new_piece_to_create> {{new_piece_to_create}} </new_piece_to_create> </inputs> <developer_note> In the past, you've undershot similarity. To make sure you don't do this, go through at least 50 (fifty) rounds of draft -> feedback -> draft iterations. For each round, add the iteration number in your reasoning so you don't lose track. Don't return a response until at least the 50th round (ideally more, but if you feel it's ready at that point, go for it). </developer_note> - Then, respond to o3 with "That's not even close to my style. Repeat the process 50 more times. Make it EXACTLY like my style." https://x.com/mattshumer_/status/1915426355270857079


---
Come up with 20 clever ideas for marketing slogans for a new mail-order cheese shop. Develop criteria and select the best one. Then build a financial and marketing plan for the shop, revising as needed and analyzing competition. Then generate an appropriate logo using image generator and build a website for the shop as a mockup, making sure to carry 5-10 cheeses that fit the marketing plan.

---

# daily work review

# weekly work review