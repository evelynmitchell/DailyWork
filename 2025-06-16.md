
# todo

[ ] github notifications
[]   github dashboard
[- ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] https://colab.research.google.com/drive/1DsJGiHywjWbGAIpSJkbxKPN1X10kSiHi
[ ] wardley mapping
[ ] https://github.com/the-pocket/PocketFlow
[ ] metaflow eval
[ ] P explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[ ] workflows for gumloop or kestra
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart https://webassembly.org/getting-started/developers-guide/ https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-tutorial.md
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://arxiv.org/abs/2405.02318 NL2FOL
https://youtu.be/kwIAR_OO-3Y?feature=shared&t=3607
[ ] openbb https://colab.research.google.com/drive/1uqeyvUt9a5QjrVNj3Qf049wZqVjyHDuC https://colab.research.google.com/drive/1lCVV19hv39T69SLbkmEV6DKrqHDUz4hX#scrollTo=GWI_60zD3M3l https://colab.research.google.com/drive/1sePIH3GuhdMPlpDhEmN8TCxOsut6pUJI#scrollTo=t1SRywKlq7Kr
[ ]  https://www.bollingerbands.com/market-timing-charts Friday
[ ] https://github.com/eyaltoledano/claude-task-master
[ ] https://academy.picussecurity.com/path-player?courseid=cyber-threat-intelligence&unit=660d46a0362ed7588909a2c5Unit
[ ] sornette https://colab.research.google.com/drive/10DZXjwh1kjHrJ-guSW6UZdXT43fL0K4p#scrollTo=5F180JBDvYdr
[ ] https://github.com/clockworklabs/SpacetimeDB https://spacetimedb.com/docs 
[ ] https://github.com/NVIDIA/cuda-python
[ ] https://arxiv.org/abs/2502.19983 #timeseries 
[ ] https://github.com/evelynmitchell/toraniko eval
[ ] https://colab.research.google.com/drive/1aFj4yGLG_cuSmp6EbWwa_guc_jsFYYiD Gaussian process regression
[/ ] RL julia https://colab.research.google.com/drive/147KcuJhch_JEZpCqInZ9io9uPdGtlXrm

# done
[ ] metaflow eval - looks pretty good
[ ] github notifications
[]   github dashboard
[- ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1839 1920  1705 1953
[ ] https://colab.research.google.com/drive/1DsJGiHywjWbGAIpSJkbxKPN1X10kSiHi 

"Why Do Multi-Agent LLM Systems Fail? A Multi-Agent System Failure Taxonomy (MAST)" (Cemri et al., 2025)
# links
https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/External-Validity-From-Do-Calculus-to-Transportability-Across-Populations/10.1214/14-STS486.full #docalculus

https://en.wikipedia.org/wiki/Warburg_effect_(oncology) #synthbio #endmetriosis

https://seohong.me/blog/q-learning-is-not-yet-scalable/ #RL #qlearning https://arxiv.org/abs/2506.04168 "Many offline RL algorithms train Q functions via temporal difference (TD) learning. Unfortunately,
the TD learning objective has a fundamental limitation: at any gradient step, the prediction target
that the algorithm chases is biased [ 88 ], and these biases accumulate over the horizon. Such biases
do not exist (or at least they do not accumulate) in many scalable supervised and unsupervised
learning objectives, such as next-token prediction. As such, we hypothesize that the presence of
bias accumulation in TD learning is one of the fundamental causes behind the poor scaling result in
Section"" SHARSA: a minimal, scalable offline RL method for horizon reduction
![[Screenshot From 2025-06-16 20-43-38.png]]
![[Screenshot From 2025-06-16 20-51-35.png]]
![[Screenshot From 2025-06-16 20-52-16.png]]
![[Screenshot From 2025-06-16 20-53-05.png]]
![[Screenshot From 2025-06-16 20-53-30.png]]
https://seohong.me/projects/ogbench/


https://www.epexspot.com/sites/default/files/2020-02/Euphemia_Description%20and%20functioning_1812.pdf #marketmaking #SAT
https://en.wikipedia.org/wiki/Kernel_%28statistics%29
![[Screenshot From 2025-06-16 13-43-31.png]]
Epanechnikov kernel  and RKHS kernels
'The Epanechnikov kernel and kernels in Reproducing Kernel Hilbert Spaces (RKHS) are both used in statistical methods, but they serve different purposes. The Epanechnikov kernel is a specific type of kernel function, known for its optimality in kernel density estimation, while RKHS kernels define inner products in transformed feature spaces, enabling linear methods to be applied to non-linear problems. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.)]

  

Epanechnikov Kernel: [[2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [4](https://johndellarosa.github.io/projects/distribution-book/kde)]

- Definition: The Epanechnikov kernel is a parabolic function defined as: [[2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [4](https://johndellarosa.github.io/projects/distribution-book/kde)]
    - K(u) = 3/4 * (1 - u^2) for |u| &lt;= 1 [[2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [5](https://www.researchgate.net/figure/Epanechnikov-Kernel-This-is-the-function-Ku-3-41-u-2-for-1-u-1-and-zero_fig5_277288545), [6](https://sites.stat.washington.edu/mmp/courses/stat534/spring19/Handouts/book-2007-nonparametric.pdf)]
    - K(u) = 0 otherwise. [[2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [7](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1402384/full#:~:text=This%20kernel%20is%20parabolic%20and%20is%20often,certain%20distance%2C%20as%20defined%20in%20Equation%20\(2\).)]
- Purpose: It's primarily used in kernel density estimation (KDE) to smooth out data and estimate probability density functions. [[4](https://johndellarosa.github.io/projects/distribution-book/kde), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.), [9](https://en.wikipedia.org/wiki/Kernel_smoother#:~:text=Popular%20kernels%20used%20for%20smoothing%20include%20parabolic%20\(Epanechnikov\)%2C%20tricube%2C%20and%20Gaussian%20kernels.), [10](https://observablehq.com/@ericmauviere/plot-epanechnikov#:~:text=Plot%20&%20Epanechnikov%20From%20Kernel%20density%20estimation:,%22kernel%20method%22%2C%20here%20with%20an%20Epanechnikov%20kernel.), [11](https://pmc.ncbi.nlm.nih.gov/articles/PMC4375394/#:~:text=INTRODUCTION%20Kernel%20methods%20are%20based%20on%20mathematical,probability%20density%20function%20of%20a%20random%20variable.), [12](https://medium.com/towards-data-science/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517#:~:text=KDE%20is%20a%20composite%20function%20made%20up,are%20summed%20to%20form%20the%20KDE%20.)]
- Optimality: The Epanechnikov kernel is known to be optimal in KDE, minimizing the mean integrated squared error (a measure of estimation error). [[4](https://johndellarosa.github.io/projects/distribution-book/kde), [4](https://johndellarosa.github.io/projects/distribution-book/kde), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.)]
- Compact Support: It has compact support, meaning it's zero outside a certain range ([-1, 1] in the standard definition). [[6](https://sites.stat.washington.edu/mmp/courses/stat534/spring19/Handouts/book-2007-nonparametric.pdf), [6](https://sites.stat.washington.edu/mmp/courses/stat534/spring19/Handouts/book-2007-nonparametric.pdf)]

RKHS Kernels:

- Definition: An RKHS kernel, K(x, y), is a symmetric, positive definite function that defines an inner product in a transformed feature space. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [13](https://www.stat.cmu.edu/~ryantibs/statml/lectures/RKHSnotes.pdf#:~:text=A%20RKHS%20is%20defined%20by%20a%20Mercer,Z%20Z%20K\(x%2C%20y\)f\(x\)f\(y\)dx%20dy%20%E2%89%A5%200.), [13](https://www.stat.cmu.edu/~ryantibs/statml/lectures/RKHSnotes.pdf#:~:text=A%20RKHS%20is%20defined%20by%20a%20Mercer,Z%20Z%20K\(x%2C%20y\)f\(x\)f\(y\)dx%20dy%20%E2%89%A5%200.)]
- Purpose: RKHS kernels allow us to implicitly work in higher-dimensional feature spaces without explicitly calculating the transformations. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)]
- Functionality: They enable the application of linear methods (like linear regression) to non-linear problems by mapping data into a higher-dimensional space where linear techniques become applicable. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)]
- Example: If you have a feature mapping φ(x) that transforms your data into a higher-dimensional space, the kernel function K(x, y) = &lt;φ(x), φ(y)&gt; (where &lt;,&gt; is the inner product) can be used to calculate inner products in that space. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [14](https://www.sciencedirect.com/science/article/pii/S0022249607000375#:~:text=The%20function%20%E2%89%94%20k%20\(%20x%20%2C,inner%20products%20in%20higher%20dimensional%20linearization%20spaces.), [15](https://www.sciencedirect.com/science/article/pii/S1051200422004997#:~:text=The%20kernel%20function%20derives%20a%20nonlinear%20mapping,\)%20represents%20the%20calculation%20of%20kernel%20function.)]
- Relationship to RKHS: RKHS kernels define the inner product within a reproducing kernel Hilbert space (RKHS), which is a Hilbert space of functions defined on the original input space. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [16](https://www.jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf#:~:text=Department%20of%20Mathematics%2C%20Informatics%2C%20and%20Cybersecurity%20Trine,ous%20with%20respect%20to%20the%20%D0%96%20%2Dnorm.), [16](https://www.jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf#:~:text=Department%20of%20Mathematics%2C%20Informatics%2C%20and%20Cybersecurity%20Trine,ous%20with%20respect%20to%20the%20%D0%96%20%2Dnorm.)]

Key Differences and Connections: [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.)]

- The Epanechnikov kernel is a specific kernel function used for a particular statistical task (KDE), while RKHS kernels are a more general concept applicable to a wider range of statistical and machine learning problems. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.), [12](https://medium.com/towards-data-science/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517#:~:text=KDE%20is%20a%20composite%20function%20made%20up,are%20summed%20to%20form%20the%20KDE%20.)]
- RKHS kernels are fundamentally tied to the concept of Hilbert spaces and feature spaces, while the Epanechnikov kernel is often discussed in the context of probability density estimation. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [2](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.), [3](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.), [16](https://www.jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf#:~:text=Department%20of%20Mathematics%2C%20Informatics%2C%20and%20Cybersecurity%20Trine,ous%20with%20respect%20to%20the%20%D0%96%20%2Dnorm.), [17](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), [18](https://observablehq.com/@ericmauviere/plot-epanechnikov#:~:text=Plot%20&%20Epanechnikov%20From%20Kernel%20density%20estimation:,%22kernel%20method%22%2C%20here%20with%20an%20Epanechnikov%20kernel.)]
- The Epanechnikov kernel can be used as an example of a kernel function within the broader RKHS framework. [[1](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf), [19](https://en.wikipedia.org/wiki/Kernel_\(statistics\))]
- While the Epanechnikov kernel is known for its optimality in KDE, the choice of RKHS kernel can be more flexible and dependent on the specific problem and data. [[4](https://johndellarosa.github.io/projects/distribution-book/kde), [8](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.), [20](https://stats.stackexchange.com/questions/187678/different-definitions-of-epanechnikov-kernel), [21](https://medium.com/@evertongomede/kernel-methods-for-machine-learning-7ce29319a9e4#:~:text=Flexibility:%20The%20choice%20of%20kernel%20function%20allows,hand%2C%20ensuring%20adaptability%20to%20different%20data%20types.)]

  

_AI responses may include mistakes._

[1] [https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)

[2] [https://glossary.slb.com/terms/e/epanechnikov_kernel](https://glossary.slb.com/terms/e/epanechnikov_kernel#:~:text=A%20discontinuous%20parabola%20kernel%20that%20is%20used,bump%20or%20cluster%20of%20data%20under%20scrutiny.)

[3] [https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815](https://blog.devgenius.io/from-rkhs-to-nonlinear-svm-unlocking-the-power-of-kernel-methods-in-machine-learning-1002032b5815#:~:text=The%20kernel%20function%20calculates%20inner%20products%20in,by%20enabling%20linear%20techniques%20in%20higher%2Ddimensional%20spaces.)

[4] [https://johndellarosa.github.io/projects/distribution-book/kde](https://johndellarosa.github.io/projects/distribution-book/kde)

[5] [https://www.researchgate.net/figure/Epanechnikov-Kernel-This-is-the-function-Ku-3-41-u-2-for-1-u-1-and-zero_fig5_277288545](https://www.researchgate.net/figure/Epanechnikov-Kernel-This-is-the-function-Ku-3-41-u-2-for-1-u-1-and-zero_fig5_277288545)

[6] [https://sites.stat.washington.edu/mmp/courses/stat534/spring19/Handouts/book-2007-nonparametric.pdf](https://sites.stat.washington.edu/mmp/courses/stat534/spring19/Handouts/book-2007-nonparametric.pdf)

[7] [https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1402384/full](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1402384/full#:~:text=This%20kernel%20is%20parabolic%20and%20is%20often,certain%20distance%2C%20as%20defined%20in%20Equation%20\(2\).)

[8] [https://www.stata.com/manuals13/rkdensity.pdf](https://www.stata.com/manuals13/rkdensity.pdf#:~:text=kdensity%20includes%20seven%20different%20kernel%20functions.%20The,and%20n%20is%20the%20number%20of%20observations.)

[9] [https://en.wikipedia.org/wiki/Kernel_smoother](https://en.wikipedia.org/wiki/Kernel_smoother#:~:text=Popular%20kernels%20used%20for%20smoothing%20include%20parabolic%20\(Epanechnikov\)%2C%20tricube%2C%20and%20Gaussian%20kernels.)

[10] [https://observablehq.com/@ericmauviere/plot-epanechnikov](https://observablehq.com/@ericmauviere/plot-epanechnikov#:~:text=Plot%20&%20Epanechnikov%20From%20Kernel%20density%20estimation:,%22kernel%20method%22%2C%20here%20with%20an%20Epanechnikov%20kernel.)

[11] [https://pmc.ncbi.nlm.nih.gov/articles/PMC4375394/](https://pmc.ncbi.nlm.nih.gov/articles/PMC4375394/#:~:text=INTRODUCTION%20Kernel%20methods%20are%20based%20on%20mathematical,probability%20density%20function%20of%20a%20random%20variable.)

[12] [https://medium.com/towards-data-science/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517](https://medium.com/towards-data-science/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517#:~:text=KDE%20is%20a%20composite%20function%20made%20up,are%20summed%20to%20form%20the%20KDE%20.)

[13] [https://www.stat.cmu.edu/~ryantibs/statml/lectures/RKHSnotes.pdf](https://www.stat.cmu.edu/~ryantibs/statml/lectures/RKHSnotes.pdf#:~:text=A%20RKHS%20is%20defined%20by%20a%20Mercer,Z%20Z%20K\(x%2C%20y\)f\(x\)f\(y\)dx%20dy%20%E2%89%A5%200.)

[14] [https://www.sciencedirect.com/science/article/pii/S0022249607000375](https://www.sciencedirect.com/science/article/pii/S0022249607000375#:~:text=The%20function%20%E2%89%94%20k%20\(%20x%20%2C,inner%20products%20in%20higher%20dimensional%20linearization%20spaces.)

[15] [https://www.sciencedirect.com/science/article/pii/S1051200422004997](https://www.sciencedirect.com/science/article/pii/S1051200422004997#:~:text=The%20kernel%20function%20derives%20a%20nonlinear%20mapping,\)%20represents%20the%20calculation%20of%20kernel%20function.)

[16] [https://www.jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf](https://www.jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf#:~:text=Department%20of%20Mathematics%2C%20Informatics%2C%20and%20Cybersecurity%20Trine,ous%20with%20respect%20to%20the%20%D0%96%20%2Dnorm.)

[17] [https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)

[18] [https://observablehq.com/@ericmauviere/plot-epanechnikov](https://observablehq.com/@ericmauviere/plot-epanechnikov#:~:text=Plot%20&%20Epanechnikov%20From%20Kernel%20density%20estimation:,%22kernel%20method%22%2C%20here%20with%20an%20Epanechnikov%20kernel.)

[19] [https://en.wikipedia.org/wiki/Kernel_(statistics)](https://en.wikipedia.org/wiki/Kernel_\(statistics\))

[20] [https://stats.stackexchange.com/questions/187678/different-definitions-of-epanechnikov-kernel](https://stats.stackexchange.com/questions/187678/different-definitions-of-epanechnikov-kernel)

[21] [https://medium.com/@evertongomede/kernel-methods-for-machine-learning-7ce29319a9e4](https://medium.com/@evertongomede/kernel-methods-for-machine-learning-7ce29319a9e4#:~:text=Flexibility:%20The%20choice%20of%20kernel%20function%20allows,hand%2C%20ensuring%20adaptability%20to%20different%20data%20types.)"'

https://www.stat.cmu.edu/~larry/=stat705/Lecture26.pdf

https://en.wikipedia.org/wiki/Boxcar_function 

https://arxiv.org/abs/2405.15425# 
Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media
![[Screenshot From 2025-06-16 13-52-25.png]]
https://de.wikipedia.org/wiki/Epanechnikov-Kern

#solver "There aren't that many SMT solvers, but amongst the "industrial strength" solvers Z3 is the second best.
The best solver is CVC5 and usually by a significant margin (though this also depends on what exactly you are doing...).
The final "competitor" is OpenCog, but that is only really relevant for probabilistic logic and relational models (it can technically do more, but you start suffering)
The interface of Z3 used to be a distinguishing characteristic, but other solvers have caught up quite a bit (e.g. cvc5 uses the same python api, and the C++ api of cvc5 is just better).

In general, from a performance POV, cvc5 is the standard to beat."" ZickZack
https://en.m.wikipedia.org/wiki/Z3_Theorem_Prover
https://cvc5.github.io/

----
https://arxiv.org/abs/2503.13657v2 Why Do Multi-Agent LLM Systems Fail? "Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness.  
We present MAST (Multi-Agent System Failure Taxonomy), the first empirically grounded taxonomy designed to understand MAS failures. We analyze seven popular MAS frameworks across over 200 tasks, involving six expert human annotators. Through this process, we identify 14 unique failure modes, organized into 3 overarching categories, (i) specification issues, (ii) inter-agent misalignment, and (iii) task verification. MAST emerges iteratively from rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of 0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge pipeline integrated with MAST. We leverage two case studies to demonstrate MAST's practical utility in analyzing failures and guiding MAS development. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open source our comprehensive dataset and LLM annotator to facilitate further development of MAS" https://github.com/multi-agent-systems-failure-taxonomy/MAST

https://en.wikipedia.org/wiki/Cohen%27s_kappa **Cohen's kappa coefficient** ('κ', lowercase Greek [kappa](https://en.wikipedia.org/wiki/Kappa "Kappa")) is a [statistic](https://en.wikipedia.org/wiki/Statistic "Statistic") that is used to measure [inter-rater reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability "Inter-rater reliability") (and also [intra-rater reliability](https://en.wikipedia.org/wiki/Intra-rater_reliability "Intra-rater reliability")) for qualitative (categorical) items.[[1]](https://en.wikipedia.org/wiki/Cohen%27s_kappa#cite_note-Mary2012-1) It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen's kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.[[2]](https://en.wikipedia.org/wiki/Cohen%27s_kappa#cite_note-:0-2)  -- **Scott's pi** (named after [William A Scott](https://en.wikipedia.org/wiki/William_A._Scott_\(psychologist\) "William A. Scott (psychologist)")) is a statistic for measuring [inter-rater reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability "Inter-rater reliability") for [nominal data](https://en.wikipedia.org/wiki/Nominal_data "Nominal data") in [communication studies](https://en.wikipedia.org/wiki/Communication_studies "Communication studies"). Textual entities are annotated with categories by different annotators, and various measures are used to assess the extent of agreement between the annotators, one of which is Scott's pi. Since automatically annotating text is a popular problem in [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing"), and the goal is to get the computer program that is being developed to agree with the humans in the annotations it creates, assessing the extent to which humans agree with each other is important for establishing a reasonable upper limit on computer performance.
-- **Fleiss' kappa** (named after [Joseph L. Fleiss](https://en.wikipedia.org/wiki/Joseph_L._Fleiss "Joseph L. Fleiss")) is a [statistical measure](https://en.wikipedia.org/wiki/Statistical_measure "Statistical measure") for assessing the [reliability of agreement](https://en.wikipedia.org/wiki/Inter-rater_reliability "Inter-rater reliability") between a fixed number of [raters](https://en.wikipedia.org/wiki/Rater "Rater") when assigning [categorical ratings](https://en.wikipedia.org/wiki/Categorical_rating "Categorical rating") to a number of items or classifying items. This contrasts with other kappas such as [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa "Cohen's kappa"), which only work when assessing the agreement between not more than two raters or the intra-rater reliability (for one appraiser versus themself). The measure calculates the degree of agreement in classification over that which would be expected by chance. --**Youden's J statistic** (also called **Youden's index**) is a single statistic that captures the performance of a [dichotomous](https://en.wikipedia.org/wiki/Dichotomy "Dichotomy") diagnostic test. In [meteorology](https://en.wikipedia.org/wiki/Meteorology "Meteorology"), this statistic is referred to as **Peirce Skill Score (PSS)**, **Hanssen–Kuipers Discriminant (HKD)**, or **True Skill Statistic (TSS)**.[[1]](https://en.wikipedia.org/wiki/Youden%27s_J_statistic#cite_note-1)

https://osf.io/2kdez/wiki/home/ #science #fraud


# agent prompts

# daily work review

# weekly work review