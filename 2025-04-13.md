
# todo


[ ] github notifications
[]   github dashboard
[ ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ ] https://github.com/the-pocket/PocketFlow
[ ] metaflow eval
[/ ] can pp
[ ] linda card
[ ] P explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[ ] OpenHands project prompts in .openhands_instructions
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets â€“ A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart https://webassembly.org/getting-started/developers-guide/ https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-tutorial.md
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://arxiv.org/abs/2405.02318 NL2FOL
https://youtu.be/kwIAR_OO-3Y?feature=shared&t=3607
[ ] openbb https://colab.research.google.com/drive/1uqeyvUt9a5QjrVNj3Qf049wZqVjyHDuC https://colab.research.google.com/drive/1lCVV19hv39T69SLbkmEV6DKrqHDUz4hX#scrollTo=GWI_60zD3M3l https://colab.research.google.com/drive/1sePIH3GuhdMPlpDhEmN8TCxOsut6pUJI#scrollTo=t1SRywKlq7Kr
[x ]  https://www.bollingerbands.com/market-timing-charts Friday
[ ] https://github.com/eyaltoledano/claude-task-master
[ ] https://academy.picussecurity.com/path-player?courseid=cyber-threat-intelligence&unit=660d46a0362ed7588909a2c5Unit
[ ] sornette https://colab.research.google.com/drive/10DZXjwh1kjHrJ-guSW6UZdXT43fL0K4p#scrollTo=5F180JBDvYdr
[ ] https://github.com/clockworklabs/SpacetimeDB https://spacetimedb.com/docs 
[ ] https://github.com/NVIDIA/cuda-python
# done

[ x] github notifications
[x]   github dashboard
# links

https://arxiv.org/abs/2410.04840 Strong Model Collapse "Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish the existance of a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1\% of the total training dataset) can still lead to model collapse: larger and larger training sets do not enhance performance. We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and feed-forward neural networks for images."

https://arxiv.org/abs/1703.10987 On the Impossibility of Supersized Machines "In recent years, a number of prominent computer scientists, along with academics in fields such as philosophy and physics, have lent credence to the notion that machines may one day become as large as humans. Many have further argued that machines could even come to exceed human size by a significant margin. However, there are at least seven distinct arguments that preclude this outcome. We show that it is not only implausible that machines will ever exceed human size, but in fact impossible."

https://aider.chat/#getting-started

https://colab.research.google.com/github/microprediction/timemachines/blob/main/examples/notebooks/wiggling.ipynb #timeseries 

https://www.youtube.com/watch?v=kUtf7KkKRmM James Turrell: You Who Look | Art + Film

https://malaga-fca-group.r-universe.dev/fcaR #conceptmap 

https://linux.die.net/man/1/flow-report #flowmapping

https://pypots.com/ #timeseries 

https://scholarworks.gvsu.edu/books/9/  Mathematical Reasoning: Writing and Proof, Version 2.1, Ted Sundstrom, Grand Valley State University
https://richardhammack.github.io/BookOfProof/

https://hub.getdbt.com/

https://www.youtube.com/playlist?list=PLw9kjlF6lD5UqaZvMTbhJB8sV-yuXu5eW #aisafety

https://marginalrevolution.com/marginalrevolution/2025/04/alignment-vs-capitalization.html

https://voiceaiandvoiceagents.com/

https://www.chainguard.dev/ #containers 

https://lawsen.substack.com/p/getting-the-most-from-deep-research #prompt 

https://github.com/marcelmarais/obsidian-mcp-server #mcp
# agent prompts

# daily work review

# weekly work review