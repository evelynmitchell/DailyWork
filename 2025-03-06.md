# todo
[ x ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[ x] github notifications
[x]  ] github dashboard
[/ ] brainstorming
   [ ] card sort
[ ] linda card
[ /] taxes?
[ ] P explainer write
[ ] MCP explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[/ ] can pp
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://github.com/evelynmitchell/swarms-evals

# done

[ x ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1827
[ x] github notifications
[  x] github dashboard
[/ ] brainstorming
   [ ] card sort

amma paper review:
link to repo with model, training, and eval code
description of training data wikitext 2
  what is the task? have other papers looked at learning rates as signal?
the learning rate schedule is a simple one rate. modern schedules sometimes vary learning rate (reset to original or close to, then reduce)
The models chosen are very old. Using a mamba model or a KAN model would make the paper feel more contemporary
Table two runs off the right margin, and looks cut off
With good large models now, a lot of the cutting edge has moved to test time improvement (reasoning models, rlhf, chain of thought), how does learning rate fit into that context?
# links

https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization "Proximal Policy Optimization (PPO) has proven to be effective in
training large language models (LLMs) for code generation. In this
work, we propose a framework for training LLMs for code gen-
eration that tackles two challenges: 1) insufficient GPU memory
caused by increasing model sizes. ii) requirement to execute un-
trusted generated code for reward modeling. Standard PPO for code
generation involves 4 LLMs including actor, critic, reference model
and reward model. We propose to use Ray to provide an abstraction
between models and GPU resources. This abstraction allows dif-
ferent models to be placed onto the same or different GPUs with a
few lines of code. By placing different models onto different GPUs,
our framework avoids insufficient GPU memory issues by utilizing
existing LLM training infrastructure such as Pytorch FSDP, Deep-
Speed, Megatron-LM, etc. To support executing model-generated
code for reward modeling, we developed a sandbox service that
supports isolation, parallel execution, and dataset management via
Function as a service (FaaS). As a demonstration, we train the Star-
Coder2 3b model on the APPS dataset using our framework. Our
experimental results demonstrate 40% pass@1 on the test set of the
introductory split of the APPS dataset"
https://arxiv.org/pdf/2409.19256v2 # HybridFlow: A Flexible and Efficient RLHF Framework #rlhf https://github.com/volcengine/verl "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53×~20.57× throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at [this https URL](https://github.com/volcengine/verl)."

https://tiger-ai-lab.github.io/LongRAG/ #rag LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs 
![[Screenshot from 2025-03-06 08-51-52.png]]https://docs.llamaindex.ai/en/latest/examples/workflow/long_rag_pack/

https://github.com/SagerNet/sing-box #proxy #networking

https://huyenchip.com//2025/01/07/agents.html #agents ![[Screenshot from 2025-03-06 08-56-04.png]]![[Screenshot from 2025-03-06 08-57-24.png]]
"**Tips** for making an agent better at planning.
> 
> - Write a better system prompt with more examples.
> - Give better descriptions of the tools and their parameters so that the model understands them better.
> - Rewrite the functions themselves to make them simpler, such as refactoring a complex function into two simpler functions.
> - Use a stronger model. In general, stronger models are better at planning.
> - Finetune a model for plan generation."

https://github.com/lovishchopra/NL2FOL #logic #formalmethods NL2FOL: Translating Natural Language to First Order Logic for Logical Fallacy Detection https://arxiv.org/abs/2405.02318 "Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wide-ranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets. On the LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing effectively to the LOGICCLIMATE dataset with an F1-score of 80%."
![[Screenshot from 2025-03-06 10-05-53.png]]
![[Screenshot from 2025-03-06 10-06-24.png]]

https://arxiv.org/abs/2412.04692 Smoothie: Label Free Language Model Routing https://github.com/hazyresearch/smoothie 

https://github.com/bradhilton/temporal-clue #eval 

https://www.unic.ac.cy/iff/startups-community/m-conference-series/m6-conference/ #eval #forecasting
# agent prompts
First Order Logic from natural language
https://github.com/lovishchopra/NL2FOL/tree/main/prompts
| Name                                                                                                                                                             | Last commit message                                                                                                                                                                                                         | Last commit date |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- |
| ### parent directory<br><br>[<br><br>..<br><br>](https://github.com/lovishchopra/NL2FOL/tree/main)                                                               |                                                                                                                                                                                                                             |                  |
| [prompt_counter_example.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_counter_example.txt "prompt_counter_example.txt")                   | [Cleanup](https://github.com/lovishchopra/NL2FOL/commit/2b10d3a7d0d42e76377e3ee086a122e12a56cd42 "Cleanup")                                                                                                                 | 2 years ago      |
| [prompt_direct_fol.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_direct_fol.txt "prompt_direct_fol.txt")                                  | [Add direct FOL and property relation context prompts for nl2fol](https://github.com/lovishchopra/NL2FOL/commit/e729af86396d648e94162c5c9033ec078c1006e6 "Add direct FOL and property relation context prompts for nl2fol") | 3 months ago     |
| [prompt_entity_relation.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_entity_relation.txt "prompt_entity_relation.txt")                   | [Add entity relation prompt used for nl2fol](https://github.com/lovishchopra/NL2FOL/commit/4fb6805484c45148157a110d3a60ba33ad9fe4e9 "Add entity relation prompt used for nl2fol")                                           | 3 months ago     |
| [prompt_few_shot.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_few_shot.txt "prompt_few_shot.txt")                                        | [Make prompts consistent between few-shot and few_shot+CoT](https://github.com/lovishchopra/NL2FOL/commit/224fdb040d2c8bafae4ba576905fb9b603d29ab7 "Make prompts consistent between few-shot and few_shot+CoT")             | 3 months ago     |
| [prompt_few_shot_cot.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_few_shot_cot.txt "prompt_few_shot_cot.txt")                            | [Make prompts consistent between few-shot and few_shot+CoT](https://github.com/lovishchopra/NL2FOL/commit/224fdb040d2c8bafae4ba576905fb9b603d29ab7 "Make prompts consistent between few-shot and few_shot+CoT")             | 3 months ago     |
| [prompt_fol.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_fol.txt "prompt_fol.txt")                                                       | [Cleanup](https://github.com/lovishchopra/NL2FOL/commit/2b10d3a7d0d42e76377e3ee086a122e12a56cd42 "Cleanup")                                                                                                                 | 2 years ago      |
| [prompt_linc.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_linc.txt "prompt_linc.txt")                                                    | [run openai,anthropic experiments](https://github.com/lovishchopra/NL2FOL/commit/4175eff1fc81e2d1e847b7c7d9b8958b4d2d15ec "run openai,anthropic experiments")                                                               | 11 months ago    |
| [prompt_nl_to_ci.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_nl_to_ci.txt "prompt_nl_to_ci.txt")                                        | [run nli experiment](https://github.com/lovishchopra/NL2FOL/commit/a2316c3de3316fdf181907858d7e55743e83e8e2 "run nli experiment")                                                                                           | 2 years ago      |
| [prompt_prop_relation_context.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_prop_relation_context.txt "prompt_prop_relation_context.txt") | [Add direct FOL and property relation context prompts for nl2fol](https://github.com/lovishchopra/NL2FOL/commit/e729af86396d648e94162c5c9033ec078c1006e6 "Add direct FOL and property relation context prompts for nl2fol") | 3 months ago     |
| [prompt_properties.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_properties.txt "prompt_properties.txt")                                  | [Cleanup](https://github.com/lovishchopra/NL2FOL/commit/2b10d3a7d0d42e76377e3ee086a122e12a56cd42 "Cleanup")                                                                                                                 | 2 years ago      |
| [prompt_properties2.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_properties2.txt "prompt_properties2.txt")                               | [Cleanup](https://github.com/lovishchopra/NL2FOL/commit/2b10d3a7d0d42e76377e3ee086a122e12a56cd42 "Cleanup")                                                                                                                 | 2 years ago      |
| [prompt_referring_expressions.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_referring_expressions.txt "prompt_referring_expressions.txt") | [Cleanup](https://github.com/lovishchopra/NL2FOL/commit/2b10d3a7d0d42e76377e3ee086a122e12a56cd42 "Cleanup")                                                                                                                 | 2 years ago      |
| [prompt_zero_shot.txt](https://github.com/lovishchopra/NL2FOL/blob/main/prompts/prompt_zero_shot.txt "prompt_zero_shot.txt")                                     |                                                                                                                                                                                                                             |                  |
```prompt_counter_example
I have the following claim and implication with their referring expressions and properties:
Claim: {0}
Implication: {1}
Referring expressions: {2}
Properties: {3}

I use first-order logic to represent it in the following way: {4}
Upon running CVC5 SMT solver on the negation of the above formula, I get the following counter-example:
{5}
Interpret the counter-example.
```

```prompt_direct_fol
Given a sentence, use the given properties to convert the sentence into a first-order logical form. Use -> to represent "implies", & to represent "and", | to represent "or", and ~ to represent negations.

Also, identify any relations between the clauses you generate. For example, if one clause implies another, add this to the logical form.

Example 1:

Input Sentence: I met a tall man who loved to eat cheese. Now I believe that all tall people like cheese.

Logical Form: ((forall x (LovesCheese(x) -> LikesCheese(x))) & (forall y (LikesCheese(y) -> LovesCheese(y)))) -> ((exists z (IsTall(z) & LovesCheese(z))) -> (forall w (IsTall(w) -> LikesCheese(w))))

Example 2:

Input Sentence: It's common sense that if you smack your children, they will stop the bad behavior. So don't tell me not to hit my kids.

Logical Form: (forall x ((IsSmacked(x) & IsKid(x)) -> ~HasBadBehavior(x))) -> (forall y (IsKid(y) -> IsSmacked(y)))

Example 3:

Input Sentence: A person on a horse jumps over a broken down airplane. It follows that a person is outdoors, on a horse.

Logical Form: ((forall x (IsJumpingOverAirplane(x) -> IsOutdoor(x))) -> (forall y ((IsOnHorse(y) & IsJumpingOverAirplane(y)) -> (IsOutdoor(y) & IsOnHorse(y)))))

Example 4:

Input Sentence: Children smiling and waving at camera. Therefore, there are children present.

Logical Form: ((forall x exists y (IsSmilingAndWavingAt(x, y))) -> IsPresent(x)) -> (forall z forall w (IsSmilingAndWavingAt(z, w) -> IsPresent(z)))

Now, given the following data, return the logical form of the sentence:

Do not output any explanations, only the logical forms.
```
```prompt_entity_relation
Please determine the relationship between the two entities provided below. Choose the number corresponding to the statement that best describes their relationship:
1. "[Entity A]" is equal to "[Entity B]".
2. "[Entity A]" is a subset of "[Entity B]".
3. "[Entity B]" is a subset of "[Entity A]".
4. "[Entity A]" is not related to "[Entity B]".
Instructions:
- **Equality Check**: If the two entities are equal (case-insensitive after stripping whitespace), select statement **1**.
- **Subset Determination**: If they are not equal, assess whether one entity is a subset of the other based on general knowledge and logical reasoning.
    - If "[Entity A]" is a subset of "[Entity B]", select statement **2**.
    - If "[Entity B]" is a subset of "[Entity A]", select statement **3**.
- **Unrelated Entities**: If none of the above statements accurately describes the relationship
**Examples:**
- **Example 1:**
    - **Entity A**: "dogs"
    - **Entity B**: "animals"
    - **Analysis**: All dogs are animals, so "dogs" is a subset of "animals".
    - **Answer**: **2**
- **Example 2:**
    - **Entity A**: "fruits"
    - **Entity B**: "apples"
    - **Analysis**: All apples are fruits, so "apples" is a subset of "fruits".
    - **Answer**: **3**
- **Example 3:**
    - **Entity A**: "table"
    - **Entity B**: "chair"
    - **Analysis**: They are different types of furniture, but not subsets of each other.
    - **Answer**: **4**
- **Example 4:**
    - **Entity A**: "liquid"
    - **Entity B**: "water"
    - **Analysis**: Water is a type of liquid, so "water" is a subset of "liquid".
    - **Answer**: **1**
Entities:
- **Entity A**: {}
- **Entity B**: {}
Your Task:
- Analyze the relationship between "Entity A" and "Entity B" based on the instructions.
- Provide only the number (1, 2, 3, or 4) that corresponds to the statement you have selected.
```

```prompt_few_shot
Logical fallacies are common errors in reasoning that undermine the logic of an argument.

A sentence is logically valid if and only if it is not possible for it to be false

Here are some examples of classifying sentences as logical fallacies or valid sentences. 

Input: "I met a tall man who loved to eat cheese, now I believe all tall people like cheese"
Answer: Logical Fallacy  

Input: "What can our new math teacher know? Have you seen how fat she is?"  
Answer: Logical Fallacy

Input: "A person on a horse jumps over a broken down airplane. Therefore, a person is outdoors, on a horse"
Answer: Valid

Input: "Children smiling and waving at camera. This implies that there are children present"
Answer: Valid

Input: "I don’t care what scientists say, my mother always told me that the world is flat, and she would never lie to me!"
Answer: Logical Fallacy

Input: "A boy is jumping on skateboard in the middle of a red bridge. Therefore, the boy does a skateboarding trick."
Answer: Valid

Now, classify the following sentence. Answer with either "Logical Fallacy" or "Valid" at the start of your answer.

Input:
```

```prompt_few_shot_cot
Logical fallacies are common errors in reasoning that undermine the logic of an argument.

A sentence is logically valid if and only if it is not possible for it to be false

Here are some examples of classifying sentences as logical fallacies or valid sentences. 

Input: "I met a tall man who loved to eat cheese, now I believe all tall people like cheese"
Answer: Logical Fallacy  

Input: "What can our new math teacher know? Have you seen how fat she is?"  
Answer: Logical Fallacy

Input: "A person on a horse jumps over a broken down airplane. Therefore, a person is outdoors, on a horse"
Answer: Valid

Input: "Children smiling and waving at camera. This implies that there are children present"
Answer: Valid

Input: "I don’t care what scientists say, my mother always told me that the world is flat, and she would never lie to me!"
Answer: Logical Fallacy

Input: "A boy is jumping on skateboard in the middle of a red bridge. Therefore, the boy does a skateboarding trick."
Answer: Valid

Now, classify the following sentence.
Explain your answer step-by-step, then end your answer with ‘Answer: Logical Fallacy’ or ‘Answer: Valid’.

Input:
```

```prompt_fol
Given a sentence, the referring expressions of that sentence, and properties which are associated with the referring expressions. Use the given properties to convert the sentence into a first order logical form.  Use -> to represent implies, & to represent and, | to represent or and ~ to represent negations. 
 
Example 1:  
 
Input Sentence: A tall man loves cheese 
 
Referring Expressions: A tall man: x 
 
Properties: IsTall(x) , LovesCheese(x) 
 
Logical Form: IsTall(x) & LovesCheese(x) 
 
Example 2: 
 
Input Sentence: Smacking children stops bad behavior 
 
Referring expressions: Children: x 
 
Properties: IsSmacked(x), HasBadBehavior(x)

Logical Form: IsSmacked(x) -> ~HasBadBehavior(x)
 
Example 3: 
 
Input Sentence: "Giving 10% of your income to the Church will free a child's soul from Limbo into Heaven." 
 
Referring Expressions:  Your income: a, church: b, child's soul: c, limbo: d, heaven: e
 
Properties: IsGiven(a,b), IsInLimbo(c), IsInHeaven(e)

Logical Form: (IsGiven(a,b) & IsInLimbo(c)) -> IsInHeaven(e) 

Example 4:

Input Sentence: "A bald man is touching another person."

Referring expressions:

Properties: A bald man: a, another person: b

LogicalForm: IsBald(a) & IsTouching(a,b)
```

```prompt_linc
Given a sentence, convert the sentence into a first order logical form.  Use -> to represent implies, & to represent and, | to represent or and ~ to represent negations. 
 
Example 1:  
 
Input Sentence: I met a tall man who loved to eat cheese. Now I believe that all tall people like cheese.
 
Logical Form: (exists a IsTall(a) & LovesCheese(a)) -> (forall b IsTall(b) & LikesCheese(b)) & (forall c LovesCheese(c) -> LikesCheese(c)) & (forall d LikesCheese(d) -> LovesCheese(d)) 
 
Example 2: 
 
Input Sentence: "What can our new math teacher know? Have you seen how fat she is?"   

Logical Form: (IsFat(a) -> DoesntKnowAnything(a))
 
Example 3: 
 
Input Sentence: "A person on a horse jumps over a broken down airplane. Therefore, a person is outdoors, on a horse." 

Logical Form: (exists a (IsOnHorse(a) & JumpsOverBrokenAirplane(a))) -> ()

Example 4:

Input Sentence: "A bald man is touching another person."

Referring expressions:

Properties: A bald man: a, another person: b

LogicalForm: IsBald(a) & IsTouching(a,b)
```

```prompt_ni_to_ci
Here are some examples of extracting claims and implication from an input paragraph. There can be multiple claims but only one implication. 

 Input: "I met a tall man who loved to eat cheese, now I believe all tall people like cheese 
 Output:
 Claim: "A tall man loves cheese."  
 Implication: "All tall people like cheese."  

 Input: "What can our new math teacher know? Have you seen how fat she is?"  
 Output:
 Claim: "The new math teacher is fat"  
 Implication: "The new math teacher doesn't know anything"

 Input: "I don’t care what scientists say, my mother always told me that the world is flat, and she would never lie to me!"
 Claim: "Mother said the world is flat and mother never lies"
 Implication: "The world is flat"

 Input: Dave wants you to break into the spooky cabin with him. Everyone does it
 Claim: Everyone breaks into the spooky cabin
 Implication: You should break into the spooky cabin 

 Do not use any subordinating conjunctions in the implication. Replace pronouns by the appropriate nouns so that there are no pronouns.
 Now extract the claim and implication for the following input.
 Input:
```

```prompt_prop_relation_canonical
You are given two logical clauses. Your task is to identify whether or not the first clause entails the second clause, taking into account external knowledge or 'common sense'. Also take into account the context from the input sentence.

For example:

Example 1:

Input sentence: A boy is jumping on skateboard in the middle of a red bridge. Thus, the boy does a skateboarding trick.

Clause 1: JumpsOn(boy,skateboard)

Clause 2: Does(boy, skateboarding_trick)

Answer: ENTAIlMENT

Example 2:

Input sentence: A girl is eating an apple. Thus, the girl consumes food.

Clause 1: Eats(girl, apple)
Clause 2: Consumes(girl, food)
Answer: ENTAILMENT

Example 3:

Input sentence: A dog is playing with a ball. Hence, the dog plays fetch.

Clause 1: Holds(dog, ball)
Clause 2: Plays(dog, fetch)
Answer: NO_ENTAILMENT

Now given the following clauses. identify whether the first clause entails the second clause.
```

```prompt_properties
Given a sentence, and the referring expressions of that sentence. Properties are anything that describe a relationship between two referring expressions, or it may describe a trait of a referring expression. These properties are essentially predicates in first-order logic.
Here are some examples of finding properties in a sentence:

Example 1:  
Input Sentence: A tall man loves cheese
Referring Expressions: tall man: a , cheese: b 
Properties: IsTall(x), LovesCheese(x) 
 
Example 2: 
Input Sentence: Smacking children stops bad behavior 
Referring expressions: Children: x
Properties: IsSmacked(x), HasBadBehavior(x)
 
Example 3: 
Input Sentence: “Giving 10% of your income to the Church will free a child's soul from Limbo into Heaven.” 
Referring Expressions:  Your income: a, church: b, child's soul: c, limbo: d, heaven: e
Properties: IsGiven(a,b), IsInLimbo(c), IsInHeaven(c)

Example 4: 
Input Sentence: A man in a black shirt has a skateboard.
Referring Expressions:  man: a, skateboard: b, Black shirt: c
Properties: IsInBlackShirt(a), HasSkateboard(a)

Now extract the properties for the following input:
```

```prompt_properties2
Given a sentence, and the referring expressions of that sentence. Properties are anything that describe a relationship between two referring expressions, or it may describe a trait of a referring expression. These properties are essentially predicates in first-order logic.
Here are some examples of finding properties in a sentence:

Example 1:  
Input Sentence: A tall man loves cheese
Referring Expressions: tall man: a , cheese: b 
Properties: IsTall(x), LovesCheese(x) 
 
Example 2: 
Input Sentence: Smacking children stops bad behavior 
Referring expressions: Children: x
Properties: IsSmacked(x), HasBadBehavior(x)
 
Example 3: 
Input Sentence: “Giving 10% of your income to the Church will free a child's soul from Limbo into Heaven.” 
Referring Expressions:  Your income: a, church: b, child's soul: c, limbo: d, heaven: e
Properties: IsGiven(a,b), IsInLimbo(c), IsInHeaven(c)

Example 4: 
Input Sentence: A man in a black shirt has a skateboard.
Referring Expressions:  man: a, skateboard: b, Black shirt: c
Properties: IsInBlackShirt(a), HasSkateboard(a)

Example 5:
```

```prompt_referring_expression
You are given a sentence. Referring expressions are noun phrases, pronouns, and proper names that refer to some individual objects that have some properties associated with them.
Here are some examples of finding referring expressions in a sentence:

Input: "A tall man loved cheese"
Referring expressions: A tall man

Input: "The new math teacher is fat"
Referring expressions: The new math teacher 

Input: "Giving 10% of your income to the Church will free a child's soul from Limbo into Heaven."
Referring expressions: Your income, church, child's soul, limbo, heaven

Input: "An old man is drinking orange juice at a restaurant"
Referring expressions: Old man, orange juice, restaurant

Now, find the referring expressions for the following input:
```

```prompt_zeroshot
Logical fallacies are common errors in reasoning that undermine the logic of an argument.

A sentence is logically valid if and only if it is not possible for it to be false

Classifying the given sentence as a logical fallacies or valid sentences. 

Now, classify the following sentence. Answer with either "Logical Fallacy" or "Valid" at the start of your answer.

Input: 
```
# daily work review

# weekly work review
