# todo
[ ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[] github notifications
[ ] github dashboard
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets â€“ A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart

# done

[ ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1631
[ ] wardley mapping
[ -] paper group ()
[x] github notifications
[x ] github dashboard

# links

https://arxiv.org/abs/2502.03387 LIMO: Less is More for Reasoning "[](https://arxiv.org/search/cs?searchtype=author&query=Liu,+P)

We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at [this https URL](https://github.com/GAIR-NLP/LIMO)." https://github.com/GAIR-NLP/LIMO 
Comment: they included the distilled training data. https://huggingface.co/datasets/GAIR/LIMO

https://cdn.openai.com/deep-research-system-card.pdf

https://brooker.co.za/blog/2025/02/05/feketes.html

https://arxiv.org/abs/2502.11098 Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available [this https URL](https://github.com/sony/talkhier)."
![[Screenshot from 2025-02-25 13-16-34.png]]
![[Screenshot from 2025-02-25 13-17-18.png]]
![[Screenshot from 2025-02-25 13-18-10.png]]


https://www.softr.io/

https://www.xano.com/

https://imtiazhumayun.github.io/grokking/ Deep Networks **Always Grok** and Here is Why

https://dataweb.usitc.gov/

https://www.cloudflare.com/application-services/products/waiting-room/

https://huggingface.co/microsoft/OmniParser-v2.0 #screenparser

https://go.dev/blog/wasmexport #wasm 

https://github.com/tesserato/CodeWeaver #markdown #code Weave your codebase into a single, navigable Markdown document

https://github.com/ksze/PeerAuth #totp  https://www.schneier.com/blog/archives/2025/02/pairwise-authentication-of-humans.html

https://olmocr.allenai.org/ #ocr https://github.com/allenai/olmocr

https://www.woj.world/ Wojciech Zaremba

https://claude.ai/share/e3ba18fe-90a1-4ee2-bc09-92c3eec7c634 Ghrist, Acquinas
# agent prompts

https://developers.cloudflare.com/workers/get-started/prompting/

# daily work review

# weekly work review
