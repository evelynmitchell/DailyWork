# todo
[ ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[] github notifications
[ ] github dashboard
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton

# done

[x] github notifications
[x ] github dashboard
[x ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ x] paper group ()
https://arxiv.org/abs/2501.18585 Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs "However, we identify a phenomenon we term *underthinking*, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems."
Comment: this is an interesting framing of a problem, but doesn't go into enough detail on how to tell when either underthinking or overthinking is occuring.

https://arxiv.org/abs/2501.18512 Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch ; Deepmind ; ref https://arxiv.org/abs/2311.08105 ![[Screenshot from 2025-01-31 09-54-01.png]]![[Screenshot from 2025-01-31 09-55-02.png]]
![[Screenshot from 2025-01-31 10-03-23.png]]
![[Screenshot from 2025-01-31 10-04-49.png]]

https://arxiv.org/abs/2501.16411 PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding https://physbench.github.io/  ; https://huggingface.co/datasets/USC-GVL/PhysBench ; new benchmark for physical understanding in VLMs which currently are only at 50% compared to humans.

https://arxiv.org/abs/2501.18009 Large Language Models Think Too Fast To Explore Effectively ; Balancing exploration and empowerments. LLMs focus too much on reducing uncertainty and not enough of empowerment. "In human learning, exploration strategies are typically categorized into three types: random exploration, uncertainty-driven exploration, and empowerment. *Random exploration* introduces stochastic noise into behaviors, enabling agents to stumble upon new information. *Uncertainty-driven exploration* prioritizes sampling actions with uncertain outcomes to reduce ambiguity and improve decision-making confidence. *Empowerment*, on the other hand, emphasizes intrinsic
rewards and open-ended discovery, driving agents to maximize possibilities rather than optimizing specific outcomes. This type of exploration aligns closely with behaviors observed in tasks like scientific research, where the goal is to uncover as many novel possibilities as possible." "Empowerment: In the context of Little Alchemy 2, empowerment translates into the players’ intrinsic desire to create elements that offer many new successful combinations.Selecting combinations that maximize future potential (e.g.,
unlocking paths to more elements). For example, the element “human” in combination with other elements leads to 83 new elements, while “alien” leads to only 1 new element. Thus, the ’human’ element is more empowering than the ’alien’ element. Brändle et al. [2] uses a neural network to predict empowerment because it effectively models the complex combinatorial relationships and potential outcomes within Little Alchemy 2. By leveraging the neural network, the method incorporates multiple factors, including the probabilities of successful combinations, the likelihood of specific results, and the intrinsic empowerment values of resulting elements. This approach ensures an accurate estimation of the empowerment value by capturing both the immediate and future potential combinations.""
Comment: Empowerment uses the future as part of the value function. How do you know a move will be fruitful for exploring?


https://arxiv.org/abs/2501.16946

[ ] App
Trying out lovable.dev
```
Create a startup validation tool aimed at technical founders. The app helps them move from initial technical ideas to validated business opportunities.

Key pages and features:

1. Home Page
- Clean, minimalist design with a clear "Start New Project" button
- List of active projects (if any) with their current validation status
- Quick idea capture box for random thoughts

2. New Project Flow
- Step 1: Technical Idea Input
  * Title field
  * Detailed description field
  * Technology stack/requirements field
  * Initial target user description
  
- Step 2: AI Analysis Dashboard
  * Technical feasibility score
  * Market opportunity assessment
  * Potential customer segments
  * Key risks and assumptions
  
- Step 3: Hypothesis Builder
  * Auto-generated initial hypotheses based on AI analysis
  * Ability to add/edit hypotheses
  * Priority ranking for testing
  * Status tracking (Untested, In Progress, Validated, Invalid)

1. Customer Discovery Guide
- Template questions for customer interviews
- Interview scheduling tracker
- Key insights capture form
- Progress dashboard showing number of interviews and key learnings

2. Project Dashboard
- Overall project status
- Next actions list
- Key metrics tracking
- Recent activities log

Style:
- Modern, clean interface
- Dark mode support
- Mobile-responsive design
- Technical aesthetic (think GitHub/VS Code)

Features:
- Authentication (optional for prototype)
- Project persistence
- Simple navigation between sections
- Clear progress indicators
- Ability to export data

Tone:
- Professional but friendly
- Technical but not overwhelming
- Focus on clarity and efficiency

```
![[Screenshot from 2025-01-31 11-07-50 1.png]]

![[Screenshot from 2025-01-31 11-08-20.png]]
Comments: I like the color scheme and higher complexity. Supabase integration looks smooth
"Enable sign-up and login with social logins"
```
-- Create a profiles table to store additional user information
create table public.profiles (
  id uuid references auth.users on delete cascade not null primary key,
  email text,
  full_name text,
  avatar_url text,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null,
  updated_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Enable RLS
alter table public.profiles enable row level security;

-- Create policies
create policy "Public profiles are viewable by everyone."
  on profiles for select
  using ( true );

create policy "Users can insert their own profile."
  on profiles for insert
  with check ( auth.uid() = id );

create policy "Users can update their own profile."
  on profiles for update
  using ( auth.uid() = id );

-- Create a trigger to handle updated_at
create or replace function handle_updated_at()
returns trigger as $$
begin
  new.updated_at = now();
  return new;
end;
$$ language plpgsql;

create trigger profiles_updated_at
  before update on profiles
  for each row
  execute procedure handle_updated_at();

```

https://console.cloud.google.com/ To set up social login with google


[ ] nvidia nim deepseek testing
```python
from openai import OpenAI

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = "$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC"
)

completion = client.chat.completions.create(
  model="deepseek-ai/deepseek-r1",
  messages=[{"role":"user","content":"Which number is larger, 9.11 or 9.8?"}],
  temperature=0.6,
  top_p=0.7,
  max_tokens=4096,
  stream=True
)

for chunk in completion:
  if chunk.choices[0].delta.content is not None:
    print(chunk.choices[0].delta.content, end="")

```
```
Can you explain Reinforcement Learning without a Value Function?
Claude sonnet
```
Yes, I can explain reinforcement learning approaches that don't rely on value functions. Let me break this down clearly.

Reinforcement Learning (RL) without value functions primarily refers to Policy Gradient methods, where the agent directly learns a policy without needing to estimate state or state-action values. Here's how it works:

Instead of learning "how good" each state or action is (which is what value functions do), policy gradient methods directly learn what action to take in each state. The policy π(a|s) gives the probability of taking action 'a' in state 's'.

The key equation that makes this work is:

$$
\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) R]
$$

Where:
- $$\theta$$ represents the policy parameters
- $$J(\theta)$$ is the expected return
- $$R$$ is the reward
- $$\pi_\theta$$ is the policy

This approach has several advantages:
1. It can handle continuous action spaces naturally
2. The policy can be stochastic, which helps with exploration
3. Sometimes it's easier to learn "what to do" directly rather than learning "how good" each state is

A classic example is REINFORCE (Monte Carlo Policy Gradient), which:
4. Samples complete trajectories from the current policy
5. Uses the actual returns to update the policy parameters
6. Adjusts the policy to make "good" actions more likely and "bad" actions less likely

Would you like me to elaborate on any particular aspect of these policy-based methods?



Deepseek-r1
```
Deepseek-r1
Reinforcement Learning (RL) without a value function focuses on directly optimizing policies or leveraging simulations/planning, bypassing the need to estimate state or action values. Here's a structured explanation:

### Key Approaches to RL Without Value Functions:

1. **Policy Gradient Methods**:
    
    - **Direct Policy Optimization**: These methods adjust policy parameters (e.g., neural network weights) to maximize expected cumulative reward using gradient ascent. The gradient is computed based on trajectories (sequences of states, actions, and rewards), without explicitly modeling state/action values.
    - **Example**: The REINFORCE algorithm uses Monte Carlo sampling to estimate gradients. While it may employ a baseline (e.g., a value function) to reduce variance, it can operate without one, relying solely on observed rewards.
2. **Evolutionary Strategies**:
    
    - **Population-Based Optimization**: Algorithms like Genetic Algorithms or Evolutionary Strategies generate diverse policies, evaluate their performance (fitness) via cumulative rewards, and iteratively select/mutate the best candidates. This trial-and-error approach avoids explicit value estimation.
3. **Model-Based Planning**:
    
    - **Simulation-Driven Decision-Making**: Methods like Monte Carlo Tree Search (MCTS) simulate future trajectories using an environment model. Actions are chosen based on the outcomes of these simulations (e.g., average rewards from rollouts), rather than precomputed value functions.
4. **Direct Policy Search**:
    
    - **Black-Box Optimization**: Techniques like hill climbing or Bayesian optimization explore the policy space directly, treating the policy as a black box and optimizing it based on reward feedback. No intermediate value estimates are used.

### Key Distinctions from Value-Based RL:

- **No Explicit Value Estimation**: These methods avoid learning ( V(s) ) (state value) or ( Q(s,a) ) (action value). Instead, they focus on improving policies or leveraging simulations.
- **Trial-and-Error Focus**: Policies are refined through interaction (real or simulated), emphasizing outcomes rather than value predictions.
- **Scalability vs. Sample Efficiency**: While policy gradients and evolutionary methods can handle high-dimensional spaces, they may require more samples than value-based methods, which generalize via value estimates.

### Use Cases:

- **Complex Environments**: When the state/action space is too large for value function approximation.
- **Non-Differentiable Systems**: Evolutionary strategies work well where gradients are hard to compute.
- **Simulation-Heavy Tasks**: Model-based planning excels in domains with accurate simulators (e.g., robotics, games).

### Limitations:

- **Sample Inefficiency**: Many trials may be needed to converge without value-guided exploration.
- **Local Optima**: Direct policy search can get stuck in suboptimal policies without global value guidance.

In essence, RL without value functions prioritizes direct policy improvement or environment interaction over value estimation, offering flexibility in scenarios where value functions are impractical to compute.
```

# links

# agent prompts

SEObot - automated entire blog SEO. - figures out my topics, keywords, knowledge tree, and more - pitches me article ideas daily over an email - generates them & syncs with my CMS - does all internal and external linking

listingbott

- works 24/7 to find all relevant directories & websites where it can list my product. - it brings me more backlinks - direct click traffic - I'm placed on new directories & if one blows up, I'm already listed there & I get a solid upside
- 
AI Agent for outreach - analyze your product - search social media - find the related influencers - write & send DMs

Email Outreach with an agent. - Connects to hubspot - Creates & manages your contacts & deals - Gets insights from all data & sends outreach emails

Galileo AI - a UI generation platform for design ideation. Takes just minutes to convert natural language to UI designs and prototypes.

AI Agent for customer support.

AI agents from

[@hcompany_ai](https://x.com/hcompany_ai)

can act as your cofounders - do research, buy the domain, make payments, build sites and more.

AI Agent is my boss now! Nova - AI Agent who acts as a Project Manager! One step closer to "The world's most AI Automated Organization" Use reply section as the waitlist, I'll invite you to the beta when it's safe to do so. Acts as a PM Keeps track of the process, organizing tasks, docs, plans, calls, releases… A typical PM

sales s a swarm of AI agents acting as an army of analysts to help them plan, prioritize, research, engage, and keep up with their customers.


# daily work review

# weekly work review