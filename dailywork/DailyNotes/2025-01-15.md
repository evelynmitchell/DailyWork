# todo
[ ] paper group ()
[ ] team meeting
[ x ] github notifications
[  ] github dashboard
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ x] claude data download

# done
[ x ] github notifications
[ x ] github dashboard
[x] https://github.com/evelynmitchell/lightning-attention
[x ] claude data download

### paper group

[x ] paper group (09:16-10:00)
https://arxiv.org/abs/2501.08313 MiniMax-01: Scaling Foundation Models with Lightning Attention; lightning attention , MoE with 32 experts and 456 billion total parameters, of
which 45.9 billion are activated for each token. RULER performance (needle-in-a-haystack) very good. Prefill latency excellent, and very scalable.
![[Pasted image 20250115092122.png]]
```
2.2.1. Lightning Attention
Lightning attention (Qin et al., 2024b,c) represents an I/O-aware, optimized implementation of
TransNormer (Qin et al., 2022a). This approach identifies the primary bottleneck in the computational
efficiency of existing linear attention mechanisms: the slow cumsum operation inherent in causal
6MiniMax-01: Scaling Foundation Models with Lightning Attention
language modeling. To alleviate this problem, Lightning Attention proposes a novel tiling technique
that effectively circumvents the cumsum operation. The key innovation lies in the strategic division of
the attention calculation into two distinct components: intra-block and inter-block computations. The
left product attention calculation is employed for intra-block operations, while the right product is
utilized for inter-block operations. This division is crucial because the intra-blocks can be significantly
reduced in size, thereby ensuring that the overall computational complexity remains linear.
```
![[Screenshot from 2025-01-15 09-23-45.png]]

https://arxiv.org/abs/2501.08332 MangaNinja: Line Art Colorization with Precise Reference Following https://github.com/ali-vilab/MangaNinjia https://johanan528.github.io/MangaNinjia/
https://arxiv.org/abs/2501.06751 Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models ; An investigation of padding tokens in T2I.![[Screenshot from 2025-01-15 09-30-56.png]]
https://arxiv.org/abs/2501.08187 A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following https://github.com/zjunlp/Instructcell 
https://arxiv.org/abs/2501.08316 Diffusion Adversarial Post-Training for One-Step Video Generation ![[Screenshot from 2025-01-15 09-36-02.png]]

https://arxiv.org/abs/2501.08328 PokerBench: Training Large Language Models to become Professional Poker Players https://huggingface.co/datasets/RZ412/PokerBench

https://arxiv.org/abs/2501.05131   
3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering https://limuloo.github.io/3DIS/
https://arxiv.org/abs/2501.08319 Enhancing Automated Interpretability with Output-Centric Feature Descriptions 
https://www.neuronpedia.org/ Open platform for Interpretability research
https://arxiv.org/abs/2501.08292 HALoGEN: Fantastic LLM Hallucinations and Where to Find Them ; a classification of hallucination types, but no data or code
https://arxiv.org/abs/2501.08167 Potential and Perils of Large Language Models as Judges of Unstructured Textual Data ; their experimental setup, and statistical analysis are worth emulating.
https://arxiv.org/abs/2501.08120 In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR https://github.com/lamm-mit/PRefLexOR a framework that combines graph reasoning with symbolic
abstraction to dynamically expand domain knowledge ; Excellent paper, the implementation is not at the right level of abstraction, but the overall framework is solid. 
```
# In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR

[Markus J. Buehler](https://arxiv.org/search/cs?searchtype=author&query=Buehler,+M+J)

> The pursuit of automated scientific discovery has fueled progress from symbolic logic to modern AI, forging new frontiers in reasoning and pattern recognition. Transformers function as potential systems, where every possible relationship remains latent potentiality until tasks impose constraints, akin to measurement. Yet, refining their sampling requires more than probabilistic selection: solutions must conform to specific structures or rules, ensuring consistency and the invocation of general principles. We present Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning), a framework that combines graph reasoning with symbolic abstraction to dynamically expand domain knowledge. Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a structured mapping, where tasks yield knowledge graphs, abstract patterns, and ultimately, final answers. Inspired by category theory, it encodes concepts as nodes and their relationships as edges, supporting hierarchical inference and adaptive learning through isomorphic representations. Demonstrations include hypothesis generation, materials design, and creative reasoning, such as discovering relationships between mythological concepts like 'thin places' with materials science. We propose a 'knowledge garden growth' strategy that integrates insights across domains, promoting interdisciplinary connections. Results with a 3-billion-parameter Graph-PReFLexOR model show superior reasoning depth and adaptability, underscoring the potential for transparent, multidisciplinary AI-driven discovery. It lays the groundwork for general autonomous reasoning solutions.
```

https://www.biorxiv.org/content/10.1101/2025.01.13.632745v1.full.pdf Life as a Function: Why Transformer Architectures Struggle to Gain Genome-Level Foundational Capabilities

### team meeting

help with git

# links

soil erosion, biochar, prairie https://worldsensorium.com/talk-dirty-to-me-summary/
The Center for Carbon Capture and Conversion (CCCC) at the University of Wyoming's (UW) School of Energy Resources https://www.uwyo.edu/ser/research/centers-of-excellence/carbon-capture-conversion/index.html

https://www.arxiv.org/abs/2501.06662 
```
# The Magnitude of Categories of Texts Enriched by Language Models

[Tai-Danae Bradley](https://arxiv.org/search/math?searchtype=author&query=Bradley,+T), [Juan Pablo Vigneaux](https://arxiv.org/search/math?searchtype=author&query=Vigneaux,+J+P)

> The purpose of this article is twofold. Firstly, we use the next-token probabilities given by a language model to explicitly define a [0,1]-enrichment of a category of texts in natural language, in the sense of Bradley, Terilla, and Vlassopoulos. We consider explicitly the terminating conditions for text generation and determine when the enrichment itself can be interpreted as a probability over texts. Secondly, we compute the Möbius function and the magnitude of an associated generalized metric space M of texts using a combinatorial version of these quantities recently introduced by Vigneaux. The magnitude function f(t) of M is a sum over texts x (prompts) of the Tsallis t-entropies of the next-token probability distributions p(−|x) plus the cardinality of the model's possible outputs. The derivative of f at t=1 recovers a sum of Shannon entropies, which justifies seeing magnitude as a partition function. Following Leinster and Schulman, we also express the magnitude function of M as an Euler characteristic of magnitude homology and provide an explicit description of the zeroeth and first magnitude homology groups.
```
# agent prompts

new one from kye
# daily work review