# todo
[x ] paper group 
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] add TheAgentCompany
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[ ] evelynmitchell/async_python_testing_tutorial
[ ] evelynmitchell/TemplateUpdateRepos
[ ] setup
[ ] deploy to GCP as well as AWS
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany

# team call
suggested indexing agents
  kye started working on making the agent stored on disk, indexable

helped with git

# done
[x] paper group
https://arxiv.org/abs/2501.04519 rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking (1) a novel code-augmented CoT data sythesis method, which performs extensive
MCTS rollouts to generate step-by-step verified reasoning trajectories used to train
the policy SLM; (2) a novel process reward model training method that avoids naïve
step-level score annotation, yielding a more effective process preference model
(PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built
from scratch and iteratively evolved to improve reasoning capabilities.
https://arxiv.org/abs/2501.04686 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics 
https://agentlaboratory.github.io/ Agent Laboratory: Using LLM Agents as Research Assistants; decent architecture of sub-tasks, good agent design and tool use, good review and reflection. Decent prompts. Q: if you are training from scratch, would the system learn good architecture? How would you incentivize learning the right architecture? "Many of the more capable models (gpt-4o, o1-mini, o1-preview) struggled with instructionfollowing during the literature review phase, and had a tendency to repeatedly use the summarize
command until the maximum phase steps have been reached, leading to a termination." - I've noticed this as well, and avoid it by having a more detailed 'reading rubric' - what function does this sentence serve? Can you extract new vocabulary? Is this sentence making a claim, and if so, what is the precise claim? Is there evidence to support the claim? Can you make an argument diagram of this including supporting claims? Have you read the citations? ...
https://arxiv.org/abs/2501.04682 Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought "Reasoning data present in pre-training corpuses does not represent the true data generation
process, especially for complex problems, which is a product of extensive latent reasoning.
Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion."" they make bold claims, but  I don't follow their core premise well.
https://arxiv.org/abs/2501.04306 LLM4SR: A Survey on Large Language Models for Scientific Research
https://arxiv.org/abs/2501.04575 InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection has good benchmarks on gui use.
https://arxiv.org/abs/2501.02772 GeAR: Generation Augmented Retrieval - decomposes information retrieval into fine grained steps and generates labels into the document portions to help with indexing.
https://arxiv.org/pdf/2501.04144 Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation
https://arxiv.org/abs/2501.04694 EpiCoder: Encompassing Diversity and Complexity in Code Generation tree-based iterative search and merge of features of code
https://arxiv.org/abs/2501.03271  DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization 99pages omg ; Principled selection of DPO kernels based on several metrics. Positive-Negative
Divergence (PND), Positive-Negative Alignment
Variance (PNAV), Triplet Alignment Tightness (TAT), and Normalized Alignment Gap
(NAG)
https://arxiv.org/abs/2501.04652 #Multi-task retriever fine-tuning for domain-specific and efficient RAG
[ x] fix discord link https://github.com/kyegomez/swarms/pull/739
# links

https://github.com/piebro/factorio-blueprint-visualizer

## k8s
dashboard https://k8slens.dev/ https://k8slens.dev/download "Lens is a Kubernetes platform that provides tools for seamless interaction with Kubernetes clusters, and an environment for secure and effective work within teams and organizations."

plugins https://krew.sigs.k8s.io/plugins/
https://krew.sigs.k8s.io/

https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/

helm charts (deploy scripts) https://helm.sh/

assistant (early) https://github.com/kemalcanbora/kube-alfred

k8d in docker github action https://github.com/helm/kind-action

## python

### performance testing
https://github.com/adalundhe/hedra https://www.hedra.dev/

### uv
https://docs.astral.sh/uv/reference/cli/#uv
https://adamj.eu/tech/2024/09/18/python-uv-development-setup/

## app dev
https://kotlinlang.org/

# agent ideas

## knowledge synthesis
https://oasislab.pubpub.org/pub/54t0y9mk/release/3
https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FTOB_x7SD9u.pdf?alt=media&token=a12f0a86-9e16-406f-b6e5-eb294ddd1770%7D%7D
https://publish.obsidian.md/joelchan-notes/discourse-graph/sources/%40strikeTypesSynthesisTheir1983

```
15 types of synthesis

- Inductive (empiricist)
    
    - 1: Generalizing over instances (p. 349)
        
    - 2: Simple theory construction (p. 349)
        
    - 3: Creating a superordinate theory (p. 349-350)
        
    - 4: Creating a worldview (p. 350)
        
- Dialectics
    
    - 5: Dialectical resolution (p. 350)
        
        - Thesis --> Antithesis --> Synthesis (e.g., Marx, Dewey)
- Kuhnian synthesis
    
    - 6: Normal science (p. 351)
        
        - Extending a paradigm to new cases (like Piaget's assimilation)
    - 7: Revolutionary science (p. 351)
        
        - Changing dominant assumptions in a field (like Piaget's accommodation)
    - 8: Overcoming incommensurable points of view (p. 352)
        
    - 9: Emergence of a paradigm
        
- Interdisciplinary
    
    - 10: Semantic synthesis (p. 352)
        
        - Creating a common language between disciplines that talk about the same concepts in different ways
    - 11: Generating interdisciplines (p. 352-353)
        
        - Develop new conceptions and approaches to a problem, modified from two or more disciplines that converge on the same problem (e.g., computational linguistics) [#Convergence](https://publish.obsidian.md/#Convergence)
    - 12: Generating multidisciplinary perspectives (p. 353)
        
        - Generating a point of view that can be used to consider/weigh data from diverse sources
- Quasi-syntheses
    
    - 13: Assessment (p.353-354)
        
        - Weighing the bulk of the evidence (aka meta-analysis / [systematic review](https://publish.obsidian.md/joelchan-notes/systematic+review)
            
        - Judgment
            
    - 14: Application and program development (p. 354)
        
    - 15: Assemblages (p. 354)
```

##  agents and markets
https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
 [ ] aap

## tacit knowledge videos
https://www.lesswrong.com/posts/SXJGSPeQWbACveJhs/the-best-tacit-knowledge-videos-on-every-subject
AI Research https://www.youtube.com/playlist?list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T
Non-profit board https://www.givewell.org/about/official-records#Boardmeetings