# todo
[  ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[ ] github notifications
[]  ] github dashboard
[/ ] can pp
[/ ] brainstorming
   [ ] card sort
[ ] linda card
[ /] taxes?
[ ] P explainer write
[ ] MCP explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets â€“ A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://github.com/evelynmitchell/swarms-evals
[ ] https://arxiv.org/abs/2405.02318 NL2FOL
https://youtu.be/kwIAR_OO-3Y?feature=shared&t=3607
[ ] openbb https://colab.research.google.com/drive/1uqeyvUt9a5QjrVNj3Qf049wZqVjyHDuC https://colab.research.google.com/drive/1lCVV19hv39T69SLbkmEV6DKrqHDUz4hX#scrollTo=GWI_60zD3M3l https://colab.research.google.com/drive/1sePIH3GuhdMPlpDhEmN8TCxOsut6pUJI#scrollTo=t1SRywKlq7Kr
[x ]  https://www.bollingerbands.com/market-timing-charts Friday

# done

[x ] github notifications
[ x] github dashboard


# links

Knitting font https://www.ravelry.com/patterns/library/knit-grotesk-abc-blanket https://typeknitting.net/Typefaces https://www.knitgrotesk.com/character-set/ https://nouvellenoire.ch/

https://www.youtube.com/watch?v=OEsmDXOW1Gk #woodcuts https://www.annexgalleries.com/inventory/detail/GOMO385/Gordon-Louis-Mortensen/Pinaleno-Mountains https://www.journeytohokusai.com/

https://arxiv.org/abs/2503.04625 START: Self-taught Reasoner with Tools ![[Screenshot from 2025-03-16 16-03-08.png]]
https://arxiv.org/abs/2410.01679 #RL #ppo VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative."

# agent prompts
Here's my code to switch off all safety filters on Gemini: [ "model" => "gemini-2.0-flash-exp-image-generation", "contents" => [ [ "role" => "user", "parts" => [ [ "text" => [$photo](https://x.com/search?q=%24photo&src=cashtag_click)['ai_edit_prompt'] ], [ "inline_data" => [ "mime_type" => "image/png", "data" => [$image](https://x.com/search?q=%24image&src=cashtag_click)_data ] ], ] ] ], "safetySettings" => [ [ "category" => "HARM_CATEGORY_HATE_SPEECH", "threshold" => "BLOCK_NONE" ], [ "category" => "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold" => "BLOCK_NONE" ], [ "category" => "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold" => "BLOCK_NONE" ], [ "category" => "HARM_CATEGORY_HARASSMENT", "threshold" => "BLOCK_NONE" ], [ "category" => "HARM_CATEGORY_CIVIC_INTEGRITY", "threshold" => "BLOCK_NONE" ] ], "generationConfig" => [ "temperature" => 1, "topP" => 0.95, "topK" => 40, "maxOutputTokens" => 8192, "responseMimeType" => "text/plain", "responseModalities" => ["image", "text"] ] ]


# daily work review

# weekly work review