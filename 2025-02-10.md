# todo
[ ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[] github notifications
[ ] github dashboard
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets â€“ A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq

# done
[ x] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1673
[x] github notifications
[ x] github dashboard
[ ] pricing
[x] awesome
[x] R1 Boulder DS
[ x] dependabot

# links

PDF tools https://github.com/qpdf/qpdf https://qpdf.readthedocs.io/en/stable/ https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md

https://arxiv.org/abs/2502.05171 Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach  https://huggingface.co/tomg-group-umd/huginn-0125 

https://arxiv.org/abs/2311.07052 Towards the Law of Capacity Gap in Distilling Language Models https://github.com/GeneZC/MiniMA

https://arxiv.org/abs/2407.16154 DDK: Distilling Domain Knowledge for Efficient Large Language Models

https://arxiv.org/abs/2502.05171 Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach ; https://github.com/seal-rg/recurrent-pretraining "We study a *novel language model architecture* that is capable of scaling test-time computation by *implicitly reasoning in latent space*. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters."

https://github.com/tylertreat/comcast simulate bad network connection

https://github.com/zhongwanjun/MemoryBank-SiliconFriend https://arxiv.org/pdf/2305.10250 "MemoryBank is a novel memory mechanism designed for Large Language Models (LLMs). With MemoryBank, models can access relevant memories, continuously evolve through memory updates, and adapt to user personalities by synthesizing past interactions. Inspired by the Ebbinghaus Forgetting Curve theory, MemoryBank incorporates a unique memory updating mechanism that mimics human-like memory behavior. This enables the AI to selectively forget or reinforce memories based on their significance and the passage of time, creating a remarkably natural memory system."

https://www.youtube.com/live/zvI4UN2_i-w 
CS 194/294-280 (Advanced LLM Agents) - Lecture 3, Yu Su
https://github.com/evelynmitchell/HippoRAG  [NeurIPS'24] HippoRAG is a novel RAG framework inspired by human long-term memory that enables LLMs to continuously integrate knowledge across external documents. RAG + Knowledge Graphs + Personalized PageRank.
https://arxiv.org/abs/2405.15071 Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalizationn https://github.com/osu-nlp-group/grokkedtransformer
![[Screenshot from 2025-02-10 18-10-25.png]]
https://arxiv.org/pdf/2305.10250 MemoryBank: Enhancing Large Language Models
with Long-Term Memory

https://soundcloud.com/bexnaj/live-imbolc-gathering-2125

https://github.com/unslothai/unsloth
https://builders.mozilla.org/project/transformer-lab/ Transformer Lab is an open source platform that allows anyone to build, tune, & run Large Language Models locally, without writing code. https://github.com/transformerlab https://transformerlab.ai/

https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf https://arxiv.org/abs/2412.19437 https://arxiv.org/abs/2402.03300 https://thezvi.substack.com/p/on-deepseeks-r1  https://www.youtube.com/watch?v=idF6TiTGYsE

https://engineering.nyu.edu/sites/default/files/2018-09/CarrJournalofFixedIncome2014.pdf


# agent prompts

# daily work review

# weekly work review
