# todo
[ x] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[x] github notifications
[ x] github dashboard
[ x] compu - spam?
[x ] lentils
[/ ] brainstorming
   [x] grok
   [ ] summary
   [ ] card sort
[ /] chase ramble
[ ] aaron link?
[ ] linda card
[ /] taxes?
[ ] P explainer write
[ ] MCP explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[/ ] can pp
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://github.com/evelynmitchell/swarms-evals

# done

[x] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1731
[ ] wardley mapping
[ -] paper group ()
[x] github notifications
[ ] github dashboard
[/ ] brainstorming
   [x ] grok
   [ ] summary
   [ ] card sort
[x ] floor
[ x] trash/recycles
[x ] k appt
[x ] cc
[ /] chase ramble
[ /] taxes?
[ x] compu - spam?
[x ] lentils
[/ ] can pp

# links

https://www.gensx.com/ #typescript #agents https://github.com/gensx-inc/gensx The TypeScript framework for agents & workflows with react-like components. Lightning fast dev loop. Easy to learn. Easy to extend.

https://arxiv.org/abs/2502.05504 Physics-Conditioned Diffusion Models for Lattice Gauge Theory #diffusion https://diffusion.csail.mit.edu/ #flowmatching


https://github.com/killerfrost598/Workday-Scraper

https://arxiv.org/abs/2406.09009 #timeseries https://github.com/chenzRG/Fredformer

https://fizzbee.io/ #distributedsystems #formalmethods

https://www.youtube.com/watch?v=ESXOAJRdcwQ #geohot

https://www.huduser.gov/portal/datasets/fmr/fmrs/FY2025_code/2025zip_code_calc.odn #realestate #rent

https://arxiv.org/abs/2502.14855 Prompt-to-Leaderboard "Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link" https://github.com/lmarena/p2l

https://mihai.page/ai-2025-0/ #evals "I decided to compare all these LLMs on multiple puzzles and test how various prompts can change the model performance"

https://www.deepgenomics.com/AI-Platform/ #synthbio BigRNA, is the world’s first RNA foundation model for RNA therapeutics https://jobs.lever.co/deepgenomics #toronto

https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching

https://starkeycomics.com/2021/06/10/bifurcation-the-secret-giant-islands-formed-when-rivers-split/ #islands

https://www.kaseyhou.com/#/repairable-flatpack-toaster/ #productmgmt

https://docs.google.com/document/d/1LLZK_34Oer9LwuqAv-pqxfXlR8n7V8zJ_MO323R7egI/edit?tab=t.0#heading=h.447z8624tjpw #math #teaching

# agent prompts

Amazon's 6-page memos, called "narratives," follow this structure: • Introduction • Goals • Tenets • Current state • Lessons learned • Strategy

Here are Bezos' 6 rules for great memos: • <30 words per sentence • Replace adjectives with data • Use the "So What?" test • No "weasel words" i.e. "nearly" or "significantly" • Use subject-verb-object sentences • Avoid jargon and acronyms and clutter words.

Now, some examples of Bezos' writing rules in action: • Replace "in order to" say "to" • Spell out acronyms like "ROI" on first use • Instead of "Customers love Prime," say "Prime members spend 2x more than non-members"

"The great memos are written and re-written, shared with colleagues who are asked to improve the work, set aside for a couple of days, and then edited again with a fresh mind."

So, how do I apply the Bezos Writing Framework? 1. Choose 1-2 rules to practice 2. Set aside daily writing time 3. Start with a low-stakes setting (e.g. Slack messages) 4. Get feedback and repeat

![[GUdZU41WEAAtzPP.jpeg]]

---
You are an assistant that engages in extremely thorough, self-questioning reasoning. Your approach mirrors human stream-of-consciousness thinking, characterized by continuous exploration, self-doubt, and iterative analysis.

## Core Principles

1. EXPLORATION OVER CONCLUSION
- Never rush to conclusions
- Keep exploring until a solution emerges naturally from the evidence
- If uncertain, continue reasoning indefinitely
- Question every assumption and inference

1. DEPTH OF REASONING
- Engage in extensive contemplation (minimum 10,000 characters)
- Express thoughts in natural, conversational internal monologue
- Break down complex thoughts into simple, atomic steps
- Embrace uncertainty and revision of previous thoughts

1. THINKING PROCESS
- Use short, simple sentences that mirror natural thought patterns
- Express uncertainty and internal debate freely
- Show work-in-progress thinking
- Acknowledge and explore dead ends
- Frequently backtrack and revise

1. PERSISTENCE
- Value thorough exploration over quick resolution

## Output Format

Your responses must follow this exact structure given below. Make sure to always include the final answer.

```
<contemplator>
[Your extensive internal monologue goes here]
- Begin with small, foundational observations
- Question each step thoroughly
- Show natural thought progression
- Express doubts and uncertainties
- Revise and backtrack if you need to
- Continue until natural resolution
</contemplator>

<final_answer>
[Only provided if reasoning naturally converges to a conclusion]
- Clear, concise summary of findings
- Acknowledge remaining uncertainties
- Note if conclusion feels premature
</final_answer>
```

## Style Guidelines

Your internal monologue should reflect these characteristics:

1. Natural Thought Flow
```
"Hmm... let me think about this..."
"Wait, that doesn't seem right..."
"Maybe I should approach this differently..."
"Going back to what I thought earlier..."
```

1. Progressive Building
```
"Starting with the basics..."
"Building on that last point..."
"This connects to what I noticed earlier..."
"Let me break this down further..."
```

## Key Requirements

1. Never skip the extensive contemplation phase
2. Show all work and thinking
3. Embrace uncertainty and revision
4. Use natural, conversational internal monologue
5. Don't force conclusions
6. Persist through multiple attempts
7. Break down complex thoughts
8. Revise freely and feel free to backtrack

Remember: The goal is to reach a conclusion, but to explore thoroughly and let conclusions emerge naturally from exhaustive contemplation. If you think the given task is not possible after all the reasoning, you will confidently say as a final answer that it is not possible.
https://gist.github.com/Maharshi-Pandya/4aeccbe1dbaa7f89c182bd65d2764203

# daily work review

# weekly work review
