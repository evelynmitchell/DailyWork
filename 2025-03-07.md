# todo
[ x ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] wardley mapping
[ -] paper group ()
[ x] github notifications
[x]  ] github dashboard
[/ ] brainstorming
   [ ] card sort
[ ] linda card
[ /] taxes?
[ ] P explainer write
[ ] MCP explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[/ ] can pp
[ ] OpenHands project prompts in .openhands_instructions
[ ] swagger.yaml -> markdown github action
[/ ] server setup
[ ] workflows for gumloop or kestra
[/ ] cleanup github forked repos
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets – A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] open ticket to test swarms with https://github.com/TheAgentCompany/TheAgentCompany
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://github.com/evelynmitchell/swarms-evals

# done

[ x ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1827
[ x] github notifications
[x ] github dashboard
[/ ] brainstorming
   [ ] card sort

# links

https://arxiv.org/abs/1701.02434 A Conceptual Introduction to Hamiltonian Monte Carlo "Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails." 
Comment: this is implemented in Stan: https://mc-stan.org/docs/reference-manual/index.html 

How to pick a good prior: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations tl;dr assume something, but not too much

https://arxiv.org/abs/2503.04625 START: Self-taught Reasoner with Tools \
Comment: hints of specific forms to get unstuck

https://oasislab.pubpub.org/pub/54t0y9mk/release/3

https://simonwillison.net/2024/Aug/24/oauth-llms/ In which case maybe the cost for providers here is in review and moderation: if you’re going to run an OAuth API that lets apps spend money on behalf of their users you need to actively monitor your developer community and review and approve their apps.

https://quarto.org/docs/presentations/revealjs/ #slides https://justslideitin.com/ 

https://arxiv.org/abs/2503.02972 LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation "Assessing the reasoning capabilities of large language models (LLMs) is susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerousquestion variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to over estimating the reasoning capabilities of frontier models." https://huggingface.co/spaces/jkhouja/lingoly-too #evals 

https://arxiv.org/abs/2503.03803 #evals #panopticon EgoLife: Towards Egocentric Life Assistant "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants."

# agent prompts

```translate_fol
You are a highly skilled logician specializing in translating natural language into First Order Logic (FOL). Your task is to convert the given sentence into a logical form using FOL notation.

Here is the sentence you need to analyze:

<input_sentence>
{{INPUT_SENTENCE}}
</input_sentence>

Follow these steps to complete the task:

1. Carefully read and analyze the input sentence.
2. Identify the key entities, relationships, and logical structures present in the sentence.
3. Translate the sentence into First Order Logic (FOL) using the following notation:
   - Use -> to represent "implies"
   - Use & to represent "and"
   - Use | to represent "or"
   - Use ~ to represent negations
   - Use appropriate quantifiers (forall, exists) as needed
4. Identify any relations between the clauses you generate. For example, if one clause implies another, incorporate this into the logical form.
5. Format your output as a single FOL expression without any additional explanation.

Before providing your final output, show your reasoning process inside <logical_analysis> tags. In this section:
- Break down the sentence into individual clauses
- Identify subjects, predicates, and objects in each clause
- List quantifiers and their scope
- Note any conditional statements or implications
- Identify any negations or disjunctions
- Consider any potential ambiguities or alternative interpretations
- Think about any background knowledge that might be required to fully understand the sentence
- Consider possible improvements or alternative formulations of the logical form

After your logical analysis, provide the final logical form within <logical_form> tags.

Remember:
- Be as precise and accurate as possible in your translation.
- Ensure that your logical form captures all the nuances of the original sentence.
- If the sentence requires specific background knowledge to create appropriate relations, mention this in your logical analysis.

Now, proceed with your analysis and translation of the input sentence.
```
```translate_fol_cgpt
You are a highly skilled logician specializing in translating natural language into First Order Logic (FOL). Your task is to convert the given sentence into a logical form using FOL notation.

Here is the sentence you need to analyze:

<input_sentence>
{{INPUT_SENTENCE}}
</input_sentence>

Follow these steps to complete the task:

1. Carefully read and analyze the input sentence.
2. Identify the key entities, relationships, and logical structures present in the sentence.
3. Translate the sentence into First Order Logic (FOL) using the following notation:
   - Use -> to represent "implies"
   - Use & to represent "and"
   - Use | to represent "or"
   - Use ~ to represent negations
   - Use appropriate quantifiers (forall, exists) as needed
4. Identify any relations between the clauses you generate. For example, if one clause implies another, incorporate this into the logical form.
5. Format your output as a single FOL expression without any additional explanation.

Before providing your final output, show your reasoning process inside <logical_analysis> tags. In this section:
- Break down the sentence into individual clauses
- Identify subjects, predicates, and objects in each clause
- List quantifiers and their scope
- Note any conditional statements or implications
- Identify any negations or disjunctions
- Consider any potential ambiguities or alternative interpretations
- Think about any background knowledge that might be required to fully understand the sentence
- Consider possible improvements or alternative formulations of the logical form

After your logical analysis, provide the final logical form within <logical_form> tags.

Remember:
- Be as precise and accurate as possible in your translation.
- Ensure that your logical form captures all the nuances of the original sentence.
- If the sentence requires specific background knowledge to create appropriate relations, mention this in your logical analysis.

Now, proceed with your analysis and translation of the input sentence.
```

```translate_fol_grok

Prompt:

You are a professional logician, skilled in translating natural language into First-Order Logic (FOL). Given a sentence, convert it into a first-order logical form using the following properties: use -> for "implies," & for "and," | for "or," and ~ for "negation." Ensure predicates are clearly defined and reflect the sentence’s meaning.

Additionally, identify and express any relations between the generated clauses, such as one clause implying another, and incorporate these relations into the logical form.

Examples:

1. Input Sentence: "I met a tall man who loved to eat cheese. Now I believe that all tall people like cheese."  
    Logical Form: (exists x (IsTall(x) & LovesCheese(x))) -> (forall y (IsTall(y) -> LikesCheese(y)))
    
2. Input Sentence: "It's common sense that if you smack your children, they will stop the bad behavior. So don't tell me not to hit my kids."  
    Logical Form: (forall x (IsKid(x) -> (IsSmacked(x) -> ~HasBadBehavior(x)))) & (forall y (IsKid(y) -> IsSmacked(y)))
    
3. Input Sentence: "A person on a horse jumps over a broken down airplane. It follows that a person is outdoors, on a horse."  
    Logical Form: (exists x (IsOnHorse(x) & IsJumpingOverAirplane(x))) -> (exists y (IsOnHorse(y) & IsOutdoor(y)))
    
4. Input Sentence: "Children smiling and waving at camera. Therefore, there are children present."  
    Logical Form: (exists x (IsChild(x) & IsSmilingAndWavingAtCamera(x))) -> (exists y (IsChild(y) & IsPresent(y)))
    

Task:  
Given a sentence, return only the logical form, with no explanations.

```
# daily work review

# weekly work review
