
# todo

[ ] github notifications
[]   github dashboard
[- ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c
[ ] https://colab.research.google.com/drive/1DsJGiHywjWbGAIpSJkbxKPN1X10kSiHi
[ ] wardley mapping
[ ] https://github.com/the-pocket/PocketFlow
[ ] metaflow eval
[ ] P explainer write
[ ] V FIX
[ ] R Alpha
[ ] value app
[ ] workflows for gumloop or kestra
[/ ] evelynmitchell/async_python_testing_tutorial - get asynchronous tests running
[ ] evelynmitchell/TemplateUpdateRepos
[ ] aap https://agentlaboratory.github.io/ and https://arxiv.org/pdf/2409.12516 A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets â€“ A New Microfoundations of GARCH model
[ ] docs on creating SEAL https://github.com/evelynmitchell/SEAL-js
[ ] write agents for https://github.com/evelynmitchell/AgentHands |
[ ] Weekly Business Review - yaml (hypothesis, dag, metrics)
[ ] modern GAN () - could work more on readme, inference, data gen, training tools
[ ] https://github.com/evelynmitchell/sophie
[ ] https://github.com/evelynmitchell/bootstrapFlywheel
[ ] set up multiagent debate training setup with phi4/ollama
[ ] https://github.com/evelynmitchell/lightning-attention CUDA, Triton
[ ] https://transformerlab.ai/docs/faq
[ ] flippable card layout https://claude.ai/chat/27b8d33a-fe75-46bb-ad15-c95f6448a329
[ ] wasm tutorial - flutter/dart https://webassembly.org/getting-started/developers-guide/ https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-tutorial.md
[ ] gensx hello world
[ ] https://diffusion.csail.mit.edu/ flow matching 
[ ] https://github.com/danielmiessler/fabric hello world, container
[ ] https://fizzbee.io/ work examples
[ ] https://arxiv.org/abs/2405.02318 NL2FOL
https://youtu.be/kwIAR_OO-3Y?feature=shared&t=3607
[ ] openbb https://colab.research.google.com/drive/1uqeyvUt9a5QjrVNj3Qf049wZqVjyHDuC https://colab.research.google.com/drive/1lCVV19hv39T69SLbkmEV6DKrqHDUz4hX#scrollTo=GWI_60zD3M3l https://colab.research.google.com/drive/1sePIH3GuhdMPlpDhEmN8TCxOsut6pUJI#scrollTo=t1SRywKlq7Kr
[ ]  https://www.bollingerbands.com/market-timing-charts Friday
[ ] https://github.com/eyaltoledano/claude-task-master
[ ] https://academy.picussecurity.com/path-player?courseid=cyber-threat-intelligence&unit=660d46a0362ed7588909a2c5Unit
[ ] sornette https://colab.research.google.com/drive/10DZXjwh1kjHrJ-guSW6UZdXT43fL0K4p#scrollTo=5F180JBDvYdr
[ ] https://github.com/clockworklabs/SpacetimeDB https://spacetimedb.com/docs 
[ ] https://github.com/NVIDIA/cuda-python
[ ] https://arxiv.org/abs/2502.19983 #timeseries 
[ ] https://github.com/evelynmitchell/toraniko eval
[ ] https://colab.research.google.com/drive/1aFj4yGLG_cuSmp6EbWwa_guc_jsFYYiD Gaussian process regression
[/ ] RL julia https://colab.research.google.com/drive/147KcuJhch_JEZpCqInZ9io9uPdGtlXrm
# done

[ x] github notifications
[x]   github dashboard
[- ] https://claude.ai/chat/d12488c0-1c09-447d-9f00-cef7cff1b64c 1870 1967 1705 1952 
[ ] https://colab.research.google.com/drive/1DsJGiHywjWbGAIpSJkbxKPN1X10kSiHi
# links

https://arxiv.org/pdf/2506.09250 
Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities"

https://huggingface.co/smolagents

https://www.crewai.com/

https://www.langchain.com/langgraph

https://www.biorxiv.org/content/biorxiv/early/2022/05/02/2021.10.31.466705.full.pdf #dopamine #RL #gamma A Spectrum of Time Horizons for Dopamine Signals

# agent prompts

# daily work review

# weekly work review